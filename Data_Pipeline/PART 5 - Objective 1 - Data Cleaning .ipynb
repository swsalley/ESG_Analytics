{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33ccbb02",
   "metadata": {},
   "source": [
    "# PART 5 - Objective 1 - Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "034d3942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aparajita/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "import scipy.stats as stats\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import glob\n",
    "\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "  \n",
    "# Create WordNetLemmatizer object \n",
    "word_lemm_obj = WordNetLemmatizer() \n",
    "stop_words = set(stopwords.words('english'))  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define missing values function to check datasets for how many missing values there are\n",
    "def missing_values(df):\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    missing_value_df = pd.DataFrame({'total # of rows': len(df),\n",
    "                                      'total # of NaN': df.isnull().sum(),\n",
    "                                      'percent missing': percent_missing})\n",
    "    missing_value_df.sort_values('percent missing', inplace=True, ascending=False)\n",
    "    return missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### Option 2: skip running the above code and read in saved datasets from the above checkpoints\n",
    "# target_dataframe_site = pd.read_csv('Saved Datasets/target_dataframe_site.csv')\n",
    "# phorizon_data = pd.read_csv('Saved Datasets/site_phorizon_pivoted_df_agg.csv')\n",
    "# sitepm_geomorph_ncss_climate_satellite_data = pd.read_csv('Saved Datasets/ssp_sitepm_ncss_geomorph_agg_prism_satellite.csv')\n",
    "# print(phorizon_data.shape , sitepm_geomorph_ncss_climate_satellite_data.shape)\n",
    "\n",
    "# #create a mapping for pedon (peiid, peiidref,siteiid,siteiidref,siteobsiid)\n",
    "# ssp_final = pd.read_csv('Saved Datasets/ssp_final.csv')\n",
    "# site_var_list = ['siteobsiid','peiidref','peiid','siteiid','siteiidref']\n",
    "# site_map_ids = ssp_final[site_var_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2821ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sitepm_geomorph_ncss_climate_satellite_data = pd.read_csv('Saved Datasets/ssp_sitepm_ncss_geomorph_agg_prism_satellite.csv')\n",
    "phorizon_data = pd.read_csv('Saved Datasets/site_phorizon_pivoted_df_agg.csv')\n",
    "#join feature data together\n",
    "\n",
    "# <<< before\n",
    "# lab_soil_data = sitepm_geomorph_ncss_climate_satellite_data.drop_duplicates(subset=['siteiid'])\n",
    "# feature_data = pd.merge(lab_soil_data, phorizon_data, how='left', on='siteiid')\n",
    "\n",
    "# >>> after\n",
    "sitepm_geomorph_ncss_climate_satellite_data.drop_duplicates(subset=['siteiid'], inplace = True)\n",
    "feature_data = pd.merge(sitepm_geomorph_ncss_climate_satellite_data, phorizon_data, how='left', on='siteiid')\n",
    "del sitepm_geomorph_ncss_climate_satellite_data, phorizon_data\n",
    "\n",
    "feature_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7203d94e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## read target data (MLRA , ecoclass information)\n",
    "target_dataframe_site = pd.read_csv('Saved Datasets/target_dataframe_site.csv')\n",
    "target_dataframe_site.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe48284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# change feature_data siteiid to int in order to merge\n",
    "feature_data['siteiid'] = feature_data['siteiid'].astype(int)\n",
    "\n",
    "### join features and target and create the final model data\n",
    "modeling_data = pd.merge(feature_data,target_dataframe_site,\n",
    "                         how='left', on='siteiid')\n",
    "modeling_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e712918",
   "metadata": {},
   "outputs": [],
   "source": [
    "### modeling data description\n",
    "# modeling_describe_df = modeling_data.describe(include='all')\n",
    "# des = modeling_describe_df.T\n",
    "# des['missing_percent'] = modeling_data.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13c37b5",
   "metadata": {},
   "source": [
    "### Drop Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2d0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_drop_df = pd.read_csv('Input Files/var_to_drop.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f9d25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_to_drop = var_to_drop_df['var_name'].tolist()\n",
    "list_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac7ce93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modeling_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71ba8b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(list_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modeling_data.shape)\n",
    "modeling_data = modeling_data.drop(columns=list_to_drop)\n",
    "print(modeling_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c8905",
   "metadata": {},
   "source": [
    "### count instead of dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666f912",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_with_pmorigin = modeling_data.columns[modeling_data.columns.str.contains('pmorigin')]\n",
    "variable_with_pmmodifier = modeling_data.columns[modeling_data.columns.str.contains('pmmodifier')]\n",
    "variable_with_pmkind = modeling_data.columns[modeling_data.columns.str.contains('pmkind')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac5960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class count_keywords_cls():\n",
    "\n",
    "    def __init__(self,dummy_var,var_name):\n",
    "        self.dummy_vars = dummy_var\n",
    "        self.var_name = var_name\n",
    "        \n",
    "    def subset_dummy_data(self,modeling_data):\n",
    "        \"\"\"\n",
    "        input : modeling data\n",
    "        output: subset of modeling with column names renamed\n",
    "        \n",
    "        This function subset the model data for variables we sent.\n",
    "        Since these are the one-hot encoded variables their naming convention is varname_category e.g. pmkind_ash\n",
    "        we only keep the category name in column\n",
    "        \"\"\"\n",
    "        self.dummy_vars = list(set(self.dummy_vars)-{self.var_name+'_nan', self.var_name+'_OTHER'})\n",
    "        print('model df = ',modeling_data.shape)\n",
    "        dummy_df = modeling_data[self.dummy_vars + ['siteiid']].copy()\n",
    "        \n",
    "#       Note: due to nature of our data set it is possible for each siteid (row) to have multiple categories\n",
    "#       so we add a suffix of ' '  for when we do a .dot product, it could be considered as a delimiter so\n",
    "#       we can distingush the cateories\n",
    "\n",
    "        dummy_df = dummy_df.add_suffix(' ')\n",
    "        dummy_df.columns = dummy_df.columns.str.replace(self.var_name+'_','')\n",
    "        print('dummy df shape after subset = ', dummy_df.shape)\n",
    "        self.dummy_df = dummy_df\n",
    "        \n",
    "    def reconstruct_var(self):\n",
    "        \"\"\"\n",
    "        takes the subseted function and reconstruct the categries from one-hot encoding by dot product of \n",
    "        columns with value 1 and column name.\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        cols_without_siteid = list(set(self.dummy_df.columns)-{'siteiid '})\n",
    "        ### To perevent memory compromise : subset to prevent kernel restart\n",
    "        self.dummy_df.loc[0:100000,self.var_name] = self.dummy_df.loc[0:100000,cols_without_siteid].dot(self.dummy_df[cols_without_siteid].columns)\n",
    "        print('dummy df shape after 1 dot = ', self.dummy_df.shape)\n",
    "        self.dummy_df.loc[100000:200000,self.var_name] = self.dummy_df.loc[100000:200000,cols_without_siteid].dot(self.dummy_df[cols_without_siteid].columns)\n",
    "        print('dummy df shape after 2 dot = ', self.dummy_df.shape)\n",
    "        self.dummy_df.loc[200000:300000,self.var_name] = self.dummy_df.loc[200000:300000,cols_without_siteid].dot(self.dummy_df[cols_without_siteid].columns)\n",
    "        print('dummy df shape after 3 dot = ', self.dummy_df.shape)\n",
    "        self.dummy_df.loc[300000:400000,self.var_name] = self.dummy_df.loc[300000:400000,cols_without_siteid].dot(self.dummy_df[cols_without_siteid].columns)\n",
    "        print('dummy df shape after 4 dot = ', self.dummy_df.shape)\n",
    "        self.dummy_df.loc[400000:,self.var_name] = self.dummy_df.loc[400000:,cols_without_siteid].dot(self.dummy_df[cols_without_siteid].columns)\n",
    "        print('dummy df shape after 5 dot = ', self.dummy_df.shape)\n",
    "        \n",
    "    def cleaning_tokenizing(self):\n",
    "        \"\"\"\n",
    "        This function lemmuniza and lower case the categories\n",
    "        \"\"\"\n",
    "    \n",
    "        self.dummy_df[self.var_name+'_lower'] = self.dummy_df[self.var_name].str.lower().astype(str)\n",
    "        self.dummy_df[self.var_name+'_lemm_lower']= self.dummy_df[self.var_name+'_lower'].apply(lambda x:word_lemm_obj.lemmatize(x))\n",
    "        dummy_df_final= self.dummy_df[['siteiid ',self.var_name+'_lemm_lower']]\n",
    "        return(dummy_df_final)\n",
    "    \n",
    "    def count_keywords_func(self,dummy_df_final):\n",
    "        \"\"\"\n",
    "        This function tokenize the \n",
    "        \"\"\"\n",
    "       \n",
    "        \n",
    "        ### data cleaning, replace - with ' '\n",
    "        dummy_df_final[self.var_name+'_lemm_lower'] = dummy_df_final[self.var_name+'_lemm_lower'].apply(lambda x:x.replace('-',' '))\n",
    "        print('getting list of keywords')\n",
    "        KEYWORDS_LST = np.unique(nltk.word_tokenize(\n",
    "            ' '.join(dummy_df_final[self.var_name+'_lemm_lower'].tolist())))\n",
    "        \n",
    "        filterd_keywords = [keyword for keyword in KEYWORDS_LST if \n",
    "                            ((keyword not in punctuation) & \n",
    "                             (not keyword.isdigit()) & \n",
    "                             (keyword not in(stop_words)))\n",
    "                           ]\n",
    "        print(' number of keywords is : ',len(filterd_keywords))\n",
    "        print(' var name = ',self.var_name)\n",
    "        for kw in filterd_keywords:\n",
    "            print ('counting keywords for : ', kw)\n",
    "            dummy_df_final[self.var_name+'_'+kw] = dummy_df_final[self.var_name+'_lemm_lower'].str.count(kw)\n",
    "        dummy_df_final.drop(columns= [self.var_name+'_lemm_lower'],inplace=True)\n",
    "        dummy_df_final.columns = dummy_df_final.columns.str.strip()\n",
    "        \n",
    "        return dummy_df_final\n",
    "    \n",
    "    def replace_in_model_df(self,count_var_df,modeling_data):\n",
    "        modeling_data_in = modeling_data.copy()\n",
    "        modeling_data_in.drop(columns= self.dummy_vars,inplace=True)\n",
    "        new_model_data = pd.merge(modeling_data_in,count_var_df,on='siteiid',how='left')\n",
    "        return (new_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ecd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_keywords_pmmodifier = count_keywords_cls(variable_with_pmmodifier,'pmmodifier')\n",
    "count_keywords_pmmodifier.subset_dummy_data(modeling_data)\n",
    "count_keywords_pmmodifier.reconstruct_var()\n",
    "tokenized_df_pmmodifier = count_keywords_pmmodifier.cleaning_tokenizing()\n",
    "count_df_pmmodifier = count_keywords_pmmodifier.count_keywords_func(tokenized_df_pmmodifier)\n",
    "print('dropping duplicated siteiid')\n",
    "count_df_pmmodifier.drop_duplicates(subset=['siteiid'],inplace=True)\n",
    "print('merge')\n",
    "new_model_data_2 = count_keywords_pmmodifier.replace_in_model_df(count_df_pmmodifier,modeling_data)\n",
    "new_model_data_2.shape\n",
    "\n",
    "# >>> after\n",
    "del count_keywords_pmmodifier, tokenized_df_pmmodifier, count_df_pmmodifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecad37",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_keywords_pmkind = count_keywords_cls(variable_with_pmkind,'pmkind')\n",
    "count_keywords_pmkind.subset_dummy_data(new_model_data_2)\n",
    "count_keywords_pmkind.reconstruct_var()\n",
    "tokenized_df_pmkind = count_keywords_pmkind.cleaning_tokenizing()\n",
    "count_df_pmkind = count_keywords_pmkind.count_keywords_func(tokenized_df_pmkind)\n",
    "count_df_pmkind.drop_duplicates(subset=['siteiid'],inplace=True)\n",
    "new_model_data_3 = count_keywords_pmkind.replace_in_model_df(count_df_pmkind,new_model_data_2)\n",
    "new_model_data_3.shape\n",
    "\n",
    "# >>> after\n",
    "del count_keywords_pmkind, tokenized_df_pmkind, count_df_pmkind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a844e",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_keywords_pmorigin = count_keywords_cls(variable_with_pmorigin,'pmorigin')\n",
    "count_keywords_pmorigin.subset_dummy_data(new_model_data_3)\n",
    "count_keywords_pmorigin.reconstruct_var()\n",
    "tokenized_df_pmorigin = count_keywords_pmorigin.cleaning_tokenizing()\n",
    "count_df_pmorigin = count_keywords_pmorigin.count_keywords_func(tokenized_df_pmorigin)\n",
    "count_df_pmorigin.drop_duplicates(subset=['siteiid'],inplace=True)\n",
    "\n",
    "# <<< before\n",
    "# new_model_data_4 = count_keywords_pmorigin.replace_in_model_df(count_df_pmorigin,new_model_data_3)\n",
    "# new_model_data_4.shape\n",
    "\n",
    "# >>> after\n",
    "modeling_data = count_keywords_pmorigin.replace_in_model_df(count_df_pmorigin,new_model_data_3)\n",
    "# modeling_data.shape\n",
    "del count_keywords_pmorigin, tokenized_df_pmorigin, count_df_pmorigin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79f1c16",
   "metadata": {},
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb052842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_claytotest(var_name,modeling_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Discretize to create 4 new flags: clay<=10 (=1 if >=0 and <=10, else 0); \n",
    "    clay10to20 (=1 if >10 and <=20, else 0); clay20to30 (=1 if >20 and <=30, else 0); \n",
    "    clay>30 (=1 if >30, else 0); missing values =0 for all new flags \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    modeling_data[var_name+'<10'] = modeling_data[var_name].apply(lambda x: \n",
    "                                                            1 if ((x <= 10)&(x >= 0)) else 0)\n",
    "    modeling_data[var_name+'_10to20'] = modeling_data[var_name].apply(lambda x: \n",
    "                                                            1 if ((x <= 20)&(x > 10)) else 0)\n",
    "    modeling_data[var_name+'_20to30'] = modeling_data[var_name].apply(lambda x: \n",
    "                                                            1 if ((x <= 30)&(x > 20)) else 0)\n",
    "    modeling_data[var_name+'>30'] = modeling_data[var_name].apply(lambda x: \n",
    "                                                            1 if (x > 30) else 0)\n",
    "    modeling_data.drop(columns=[var_name],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "claytotes_vars = modeling_data.columns[modeling_data.columns.str.contains('claytotest_')]\n",
    "for claytotest_var in claytotes_vars:\n",
    "    print(claytotest_var)\n",
    "    discretize_claytotest(claytotest_var, modeling_data)\n",
    "    \n",
    "# >>> after\n",
    "del claytotes_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discritize_fragvoltot(var_name,modeling_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Discretize to create 2 new flags: frag<=10 (=1 if >=0 and <=10, else 0); \n",
    "    frag>10 (=1 if >10; else 0); \n",
    "    missing values =0 for all new flags\n",
    "    \n",
    "    \"\"\"\n",
    "    modeling_data[var_name+'<10'] = modeling_data[var_name].apply(lambda x: \n",
    "                                                            1 if ((x <= 10)&(x >= 0)) else 0)\n",
    "    modeling_data[var_name+'>10'] = modeling_data[var_name].apply(lambda x: \n",
    "                                                            1 if (x > 10) else 0)\n",
    "    modeling_data.drop(columns=[var_name],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adeeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fragvoltot_vars = modeling_data.columns[modeling_data.columns.str.contains('fragvoltot')]\n",
    "for fragvoltot_var in fragvoltot_vars:\n",
    "    print(fragvoltot_var)\n",
    "    discritize_fragvoltot(fragvoltot_var,modeling_data)\n",
    "    \n",
    "# >>> after\n",
    "del fragvoltot_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63eb0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discritize_phfield(var_name,modeling_data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Discretize using scale to right to create 11 new flags; \n",
    "    missing values are 0 for all flags\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    modeling_data[var_name+'_strong_acidic'] = modeling_data[var_name].apply(lambda x: \n",
    "                                         1 if (x <= 5.5) else 0)\n",
    "    modeling_data[var_name+'_moderate_acidic'] = modeling_data[var_name].apply(lambda x: \n",
    "                                    1 if ((x <= 6)&(x >= 5.6)) else 0)\n",
    "    modeling_data[var_name+'_slight_acidic'] = modeling_data[var_name].apply(lambda x: \n",
    "                                    1 if ((x <= 6.5)&(x >= 6.1)) else 0)\n",
    "    modeling_data[var_name+'_neutral'] = modeling_data[var_name].apply(lambda x: \n",
    "                                    1 if ((x <= 7.3)&(x >= 6.6)) else 0)\n",
    "    modeling_data[var_name+'_slight_alkaline'] = modeling_data[var_name].apply(lambda x: \n",
    "                                    1 if ((x <= 7.8)&(x >= 7.4)) else 0)\n",
    "    modeling_data[var_name+'_moderate_alkaline'] = modeling_data[var_name].apply(lambda x: \n",
    "                                    1 if ((x <= 8.4)&(x >= 7.9)) else 0)\n",
    "    modeling_data[var_name+'_strong_alkaline'] = modeling_data[var_name].apply(lambda x: \n",
    "                                    1 if (x >= 8.5) else 0)    \n",
    "    modeling_data.drop(columns=[var_name],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1647fadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "phfild_vars = modeling_data.columns[modeling_data.columns.str.contains('phfield')]\n",
    "for phfild_var in phfild_vars:\n",
    "    print(phfild_var)\n",
    "    discritize_phfield(phfild_var,modeling_data)\n",
    "    \n",
    "# >>> after\n",
    "del phfild_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da530a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_data['phfield_0_moderate_acidic'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13691d26",
   "metadata": {},
   "source": [
    "### Replace lat, long , elev with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7fc939",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_long_elev_data = pd.read_csv('Input Files/siteiid_lat_long_elev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4537fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "geographic_loc_info = ['latstddeci','latstddecimaldegrees','longstddec','longstddecimaldegrees','elev']\n",
    "modeling_data.drop(columns = geographic_loc_info,inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cf3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = lat_long_elev_data[['siteiid','latstddeci','longstddec','mn75_grd']].drop_duplicates()\n",
    "geo_df.rename(columns={'mn75_grd':'elev'},inplace=True)\n",
    "geo_df.shape, geo_df['siteiid'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5fba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data['siteiid'] = modeling_data['siteiid'].astype(int) #make data type int to match geo_df siteiid then change back to string later\n",
    "\n",
    "# <<< before\n",
    "# modeling_data_replace_geo = pd.merge(modeling_data,\n",
    "#                                      geo_df,\n",
    "#                                      on='siteiid',how='left')\n",
    "\n",
    "# >>> after\n",
    "modeling_data = pd.merge(modeling_data,\n",
    "                         geo_df,\n",
    "                         on='siteiid',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734a8fd",
   "metadata": {},
   "source": [
    "### Fill in Null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725edfb",
   "metadata": {},
   "source": [
    "#### Fill with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb869ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_fill_zero_df = pd.read_csv('Input Files/var_to_fill_zero_data.csv')\n",
    "missing_var_list = var_to_fill_zero_df['var_name'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cdbb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data[missing_var_list] = modeling_data[\n",
    "    missing_var_list].apply(lambda x: x.fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3848b05e",
   "metadata": {},
   "source": [
    "#### Fill with median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1c1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_fill_with_median = ['hzdept_0','hzdept_10','hzdept_20','hzdept_30','hzdept_40','hzdept_50','hzdept_60',\n",
    " 'hzdept_70','hzdept_80','hzdept_90','hzdept_100','hzdept_110','hzdept_120',\n",
    "'hzdepb_0','hzdepb_10','hzdepb_20','hzdepb_30','hzdepb_40','hzdepb_50','hzdepb_60',\n",
    "'hzdepb_70','hzdepb_80','hzdepb_90','hzdepb_100','hzdepb_110','hzdepb_120']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de16b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data[var_to_fill_with_median] = modeling_data[\n",
    "    var_to_fill_with_median].apply(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2999fb",
   "metadata": {},
   "source": [
    "### Create Flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27179596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature engineer flag noncarbclay (=0 if Nan; 1 otherwise)\n",
    "\n",
    "flag_var = ['noncarbclaywtavg','claytotwtavg',\n",
    "            'cec7clayratiowtavg','pmorder','psctopdepth']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in flag_var:\n",
    "    modeling_data.loc[modeling_data[var].notna(),var] = 1\n",
    "    modeling_data.loc[modeling_data[var].isna(),var] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97011a63",
   "metadata": {},
   "source": [
    "### Group together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0ee8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's group these together into a new flag \n",
    "# statextsflag_0to30 (=1 if _0,10,20, or 30 are 1, else 0)\n",
    "\n",
    "stratextsflag_vars = ['stratextsflag_0','stratextsflag_10',\n",
    "                      'stratextsflag_20','stratextsflag_30']\n",
    "\n",
    "modeling_data[stratextsflag_vars] = modeling_data[\n",
    "    stratextsflag_vars].apply(lambda x: x.fillna(0))\n",
    "\n",
    "modeling_data['statextsflag_0to30'] = modeling_data[stratextsflag_vars].any(axis=1).map(\n",
    "    {True:1,False:0})\n",
    "\n",
    "modeling_data.drop(columns=stratextsflag_vars,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group _0 to _70 into a new flag horcolorvflag_0to70 \n",
    "#                       (=1 if any of _0 to _70 =1; else 0); \n",
    "# this will convert missing to 0 as well\n",
    "\n",
    "horcolorvflag_vars = ['horcolorvflag_0', 'horcolorvflag_10', 'horcolorvflag_20', \n",
    "                      'horcolorvflag_30', 'horcolorvflag_40', 'horcolorvflag_50',\n",
    "                      'horcolorvflag_60', 'horcolorvflag_70']\n",
    "\n",
    "modeling_data[horcolorvflag_vars] = modeling_data[\n",
    "    horcolorvflag_vars].apply(lambda x: x.fillna(0))\n",
    "\n",
    "modeling_data['horcolorvflag_0to70'] = modeling_data[horcolorvflag_vars].any(axis=1).map(\n",
    "    {True:1,False:0})\n",
    "\n",
    "modeling_data.drop(columns=horcolorvflag_vars,inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f85ca2",
   "metadata": {},
   "source": [
    "### OTHER CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d245f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop count of Nan variables\n",
    "nan_vars = modeling_data.columns[modeling_data.columns.str.endswith('_nan')]\n",
    "# drop variables that have _OTHER and are less than 0.01 of column\n",
    "other_vars = [\n",
    "'obsmethod_0_OTHER',\n",
    "'obsmethod_100_OTHER',\n",
    "'obsmethod_70_OTHER',\n",
    "'obsmethod_30_OTHER',\n",
    "'obsmethod_80_OTHER',\n",
    "'obsmethod_120_OTHER',\n",
    "'obsmethod_110_OTHER',\n",
    "'obsmethod_20_OTHER',\n",
    "'obsmethod_90_OTHER',\n",
    "'obsmethod_40_OTHER',\n",
    "'obsmethod_50_OTHER'\n",
    "]\n",
    "\n",
    "modeling_data.drop(columns=nan_vars,inplace=True)\n",
    "modeling_data.drop(columns=other_vars,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a8df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill NAN values with zeros for the following features\n",
    "fill_with_zero_feat = [\n",
    "    'Feature_Type_Anthropogenic Feature', 'Feature_Type_Landform', 'Feature_Type_Landscape', \n",
    "    'Feature_Type_Microfeature', 'Feature_alluvial fan', 'Feature_coastal plain', 'Feature_drainageway', \n",
    "    'Feature_flood plain', 'Feature_foothills', 'Feature_ground moraine', 'Feature_hill', 'Feature_hills', \n",
    "    'Feature_hillslope', 'Feature_interfluve', 'Feature_intermontane basin', 'Feature_lake plain', \n",
    "    'Feature_mountain', 'Feature_mountain slope', 'Feature_mountains', 'Feature_other', 'Feature_outwash plain',\n",
    "    'Feature_piedmont', 'Feature_plain', 'Feature_plains', 'Feature_plateau', 'Feature_ridge', \n",
    "    'Feature_river valley', 'Feature_stream terrace', 'Feature_terrace', 'Feature_till plain', \n",
    "    'Feature_upland', 'Feature_valley', 'obsmethod_0_Bucket Auger', 'obsmethod_50_Large Pit or Quarry', \n",
    "    'obsmethod_110_Push Tube', 'obsmethod_10_Shovel Slice', 'obsmethod_20_Push Tube', 'obsmethod_30_Small Pit', \n",
    "    'obsmethod_20_Small Pit', 'obsmethod_80_Bucket Auger', 'hzname_20_OTHER', 'hzname_70_OTHER',\n",
    "    'obsmethod_60_Trench', 'hzname_10_OTHER', 'obsmethod_120_Bucket Auger', 'obsmethod_40_Small Pit',\n",
    "    'obsmethod_0_Cut', 'desgnmaster_50_OTHER', 'obsmethod_10_Small Pit', 'obsmethod_20_Cut', \n",
    "    'obsmethod_10_Push Tube', 'obsmethod_70_Cut', 'obsmethod_50_Push Tube', 'obsmethod_30_Shovel Slice', \n",
    "    'obsmethod_90_Screw Auger', 'obsmethod_50_Small Pit', 'obsmethod_10_Large Pit or Quarry', 'obsmethod_50_Cut',\n",
    "    'hzname_0_OTHER', 'obsmethod_120_Cut', 'obsmethod_110_Trench', 'obsmethod_90_Push Tube', \n",
    "    'obsmethod_20_Large Pit or Quarry', 'obsmethod_40_Shovel Slice', 'obsmethod_10_Trench', 'hzname_90_OTHER', \n",
    "    'obsmethod_120_Trench', 'obsmethod_100_Small Pit', 'obsmethod_0_Small Pit', 'obsmethod_0_Shovel Slice', \n",
    "    'obsmethod_120_Push Tube', 'obsmethod_30_Large Pit or Quarry', 'obsmethod_40_Cut', \n",
    "    'obsmethod_120_Large Pit or Quarry', 'obsmethod_30_Bucket Auger', 'obsmethod_60_Push Tube', \n",
    "    'obsmethod_20_Shovel Slice', 'obsmethod_30_Push Tube', 'obsmethod_70_Trench', 'obsmethod_0_Trench',\n",
    "    'obsmethod_60_Cut', 'obsmethod_40_Large Pit or Quarry', 'hzname_40_OTHER', 'obsmethod_80_Trench',\n",
    "    'hzname_60_OTHER', 'obsmethod_100_Trench', 'hzname_120_OTHER', 'obsmethod_100_Cut', 'obsmethod_80_Small Pit', \n",
    "    'desgnmaster_0_OTHER', 'obsmethod_110_Bucket Auger', 'obsmethod_60_Small Pit', 'obsmethod_90_Trench', \n",
    "    'obsmethod_90_Cut', 'desgnmaster_110_OTHER', 'desgnmaster_80_OTHER', 'obsmethod_90_Bucket Auger', \n",
    "    'desgnmaster_100_OTHER', 'obsmethod_30_Trench', 'obsmethod_80_Large Pit or Quarry', \n",
    "    'desgnmaster_10_OTHER', 'obsmethod_10_Bucket Auger', 'obsmethod_40_Trench',\n",
    "    'obsmethod_110_Large Pit or Quarry', 'obsmethod_60_Bucket Auger', 'hzname_80_OTHER', 'hzname_50_OTHER', \n",
    "    'obsmethod_50_Bucket Auger', 'obsmethod_70_Large Pit or Quarry', 'obsmethod_70_Push Tube', \n",
    "    'desgnmaster_40_OTHER', 'obsmethod_70_Small Pit', 'obsmethod_110_Small Pit', 'obsmethod_10_Cut',\n",
    "    'obsmethod_80_Cut', 'obsmethod_60_Large Pit or Quarry', 'desgnmaster_90_OTHER', 'obsmethod_110_Cut', \n",
    "    'obsmethod_20_Bucket Auger', 'desgnmaster_20_OTHER', 'obsmethod_100_Bucket Auger', 'obsmethod_80_Screw Auger',\n",
    "    'obsmethod_20_Trench', 'obsmethod_100_Large Pit or Quarry', 'obsmethod_50_Shovel Slice',\n",
    "    'desgnmaster_70_OTHER', 'obsmethod_10_OTHER', 'obsmethod_60_OTHER', 'obsmethod_80_Push Tube',\n",
    "    'obsmethod_0_Large Pit or Quarry', 'desgnmaster_30_OTHER', 'obsmethod_40_Bucket Auger', \n",
    "    'obsmethod_90_Large Pit or Quarry', 'hzname_100_OTHER', 'obsmethod_50_Trench', 'hzname_110_OTHER', \n",
    "    'obsmethod_120_Small Pit', 'obsmethod_70_Bucket Auger', 'obsmethod_90_Small Pit', 'obsmethod_0_Push Tube',\n",
    "    'obsmethod_100_Screw Auger', 'obsmethod_100_Push Tube', 'desgnmaster_60_OTHER', 'hzname_30_OTHER', \n",
    "    'obsmethod_30_Cut', 'obsmethod_40_Push Tube']\n",
    "\n",
    "\n",
    "modeling_data[fill_with_zero_feat] = modeling_data[\n",
    "    fill_with_zero_feat].apply(lambda x: x.fillna(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33df405",
   "metadata": {},
   "source": [
    "### Join with Veg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00f029",
   "metadata": {},
   "source": [
    "#### Create reference table for plant/trees\n",
    "Need this table in order to join the vegplot and plant datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de922fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegetation data\n",
    "windbreakrowdata = pd.read_csv('Input Files/windbreakrowdata.txt', delimiter=\"|\")\n",
    "plottreeinventory = pd.read_csv('Input Files/plottreeinventory.txt', delimiter=\"|\")\n",
    "plotplantinventory = pd.read_csv('Input Files/plotplantinventory.txt', delimiter=\"|\")\n",
    "plottreesiteindexsummary = pd.read_csv('Input Files/plottreesiteindexsummary.txt', delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3223d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_1 = windbreakrowdata[['plantiidref', 'vegplotiidref']]\n",
    "pv_2 = plottreesiteindexsummary[['plantiidref', 'vegplotiidref']]\n",
    "pv_3 = plottreeinventory[['plantiidref', 'vegplotiidref']]\n",
    "pv_4 = plotplantinventory[['plantiidref', 'vegplotiidref']]\n",
    "\n",
    "frames = [pv_1, pv_2, pv_3, pv_4]\n",
    "pv_table = pd.concat(frames)\n",
    "pv_table.drop_duplicates(inplace=True)\n",
    "del windbreakrowdata, plottreeinventory, plotplantinventory, plottreesiteindexsummary\n",
    "del pv_1, pv_2, pv_3, pv_4\n",
    "\n",
    "# drop duplicates\n",
    "# pv_table_final = pv_table.drop_duplicates()\n",
    "\n",
    "pv_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52795477",
   "metadata": {},
   "source": [
    "#### cleanse vegplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7926a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "vegplot = pd.read_csv('Input Files/vegplot.txt', delimiter=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867cb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview raw data\n",
    "vegplot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "missing_values(vegplot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a6482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep variables and drop the rest\n",
    "vegplot = vegplot[['vegplotiid',\n",
    "                  'soilprofileindicator',\n",
    "                  'alkalineaffected',\n",
    "                  'understorydescindicator',\n",
    "                  'mensurationdataindicator',\n",
    "                  'siteobsiidref']]\n",
    "vegplot.drop_duplicates(inplace=True)\n",
    "\n",
    "# vegplot_final = vegplot_v2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6fa892",
   "metadata": {},
   "source": [
    "#### plant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plant = pd.read_csv('Input Files/plant.txt', delimiter=\"|\", encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625c3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview raw plant dataset\n",
    "plant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ff8780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "missing_values(plant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1ac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop variables that are more than 70% missing and/or are not useful for analysis\n",
    "plant = plant.drop(columns = ['obterm',\n",
    "                             'plantdbiidref',\n",
    "                             'grpiidref', \n",
    "                             'objwlupdated',\n",
    "                             'objuseriidref',\n",
    "                             'recwlupdated',\n",
    "                             'recuseriidref',\n",
    "                             'plantsubspecies',\n",
    "                             'plantvariety'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885cff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plant.drop_duplicates(inplace=True)\n",
    "plant.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e21d9",
   "metadata": {},
   "source": [
    "#### pv + vegplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9634e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_vegplot = pd.merge(pv_table, \n",
    "                    vegplot, \n",
    "                    how='left', \n",
    "                    left_on=['vegplotiidref'], \n",
    "                    right_on=['vegplotiid'],\n",
    "                    suffixes=('_pv','_vegplot'))\n",
    "\n",
    "pv_vegplot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17409ea",
   "metadata": {},
   "source": [
    "#### +plant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67abb4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_vegplot_plant = pd.merge(pv_vegplot, \n",
    "                    plant, \n",
    "                    how='inner', \n",
    "                    left_on=['plantiidref'], \n",
    "                    right_on=['plantiid'],\n",
    "                    suffixes=('_pv','_plant'))\n",
    "\n",
    "pv_vegplot_plant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d20ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_vegplot_plant.drop(columns = ['plantiidref',\n",
    "                                'vegplotiidref',\n",
    "                                'vegplotiid',\n",
    "                                'plantiid'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef59732",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_vegplot_plant.drop_duplicates(inplace=True)\n",
    "pv_vegplot_plant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653c1e2b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pv_vegplot_plant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffde12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing plantsciname with plantnatvernm\n",
    "pv_vegplot_plant['plantsciname'].fillna(pv_vegplot_plant['plantnatvernm'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527f9e7a",
   "metadata": {},
   "source": [
    "##### One hot encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3159bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one hot encode - get variables\n",
    "plantsciname = pv_vegplot_plant['plantsciname'].value_counts().to_frame()\n",
    "features = plantsciname[plantsciname['plantsciname'] > 5000].reset_index()\n",
    "features = features.drop(columns = 'plantsciname')\n",
    "features = features.rename(columns={\"index\": \"Features\"})\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e0c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_vegplot_plant_v2 = pv_vegplot_plant.drop(columns = ['plantsym', \n",
    "                                                       'plantnatvernm',\n",
    "                                                       'plantgenus',\n",
    "                                                       'plantspecies'])\n",
    "\n",
    "pv_vegplot_plant_v3 = pd.merge(pv_vegplot_plant_v2, \n",
    "                               features, \n",
    "                               how='left', \n",
    "                               left_on=['plantsciname'], \n",
    "                               right_on=['Features'])\n",
    "pv_vegplot_plant_v3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1a947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in NaN in column Features with \"Other\"\n",
    "values = {'Features': 'Other'}\n",
    "pv_vegplot_plant_v3 = pv_vegplot_plant_v3.fillna(value=values)\n",
    "pv_vegplot_plant_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69369ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "pv_vegplot_plant_v4 = pv_vegplot_plant_v3[['siteobsiidref']].join(pd.get_dummies(pv_vegplot_plant_v3['Features']).add_prefix('PlantName_')).groupby('siteobsiidref').max().reset_index()\n",
    "pv_vegplot_plant_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de04a814",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_vegplot_plant_v4['siteobsiidref'] = pv_vegplot_plant_v4['siteobsiidref'].astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c9678",
   "metadata": {},
   "source": [
    "##### join siteid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a4b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a mapping for pedon (peiid, peiidref,siteiid,siteiidref,siteobsiid)\n",
    "ssp_final = pd.read_csv('Saved Datasets/ssp_final.csv')\n",
    "\n",
    "# <<< before\n",
    "# site_var_list = ['siteobsiid','peiidref','peiid','siteiid','siteiidref']\n",
    "\n",
    "# >>> after\n",
    "site_var_list = ['siteobsiid','siteiid']\n",
    "\n",
    "site_map_ids = ssp_final[site_var_list]\n",
    "site_ids = site_map_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953482cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <<< before\n",
    "# site_ids.drop(columns = ['peiidref', 'peiid', 'siteiidref'], inplace=True)\n",
    "# site_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change to integer in order to join\n",
    "pv_vegplot_plant_v4['siteobsiidref'] = pv_vegplot_plant_v4['siteobsiidref'].astype(int)\n",
    "\n",
    "plantname = pd.merge(pv_vegplot_plant_v4,\n",
    "                           site_ids,\n",
    "                           how='inner',\n",
    "                           left_on=['siteobsiidref'], \n",
    "                           right_on=['siteobsiid'])\n",
    "plantname.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786bc19c",
   "metadata": {},
   "source": [
    "##### collapse on siteiid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plantname_final = plantname.groupby('siteiid').max().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7204a7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plantname_final['siteiid'] = plantname_final['siteiid'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2640cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plantname_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34f92e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plantname_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ddf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plantname_final.drop(columns='siteobsiid',inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11be063",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data_veg = modeling_data.merge(plantname_final, how='left', on = 'siteiid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caf46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data.shape,modeling_data_veg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1295e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_veg_vars = list(set(list(plantname_final))-{'siteiid'})\n",
    "modeling_data_veg[fill_veg_vars] = modeling_data_veg[\n",
    "    fill_veg_vars].apply(lambda x: x.fillna(0))\n",
    "\n",
    "modeling_data = modeling_data_veg.copy()\n",
    "del modeling_data_veg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04baabcf",
   "metadata": {},
   "source": [
    "### Drop Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0b075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop texcl variables because the information is already repeating in texture varibles with less missing %\n",
    "variable_with_texcl = modeling_data.columns[modeling_data.columns.str.startswith('texcl')]\n",
    "modeling_data.drop(columns=variable_with_texcl,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop these extra variables\n",
    "drop_vars = [          \n",
    "'rupresblkmst_80_Extremely firm','rupresblkmst_90_Extremely firm','rupresblkmst_100_Extremely firm',\n",
    "'rupresblkmst_110_Extremely firm','rupresblkmst_120_Extremely firm',\n",
    "'earthcov_1_Marshland',\n",
    "'PlantName_Abies grandis',\n",
    "'PlantName_Larix occidentalis',\n",
    "'PlantName_Pseudotsuga menziesii var. glauca',\n",
    "'PlantName_Tsuga heterophylla',\n",
    "'pmorigin_andesite',\n",
    "'pmorigin_gneiss',\n",
    "'pmorigin_granitoid',\n",
    "'pmorigin_metasedimentary',\n",
    "'pmorigin_mudstone',\n",
    "'pmorigin_tuff',\n",
    "'siteobsiidref']\n",
    "modeling_data.drop(columns=drop_vars,inplace=True)\n",
    "modeling_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop_less_frequent (less than 1%):\n",
    "less_freq =   ['pmgroupnam_till',\n",
    "    'earthcov_1_Other tree cover', 'earthcov_1_Savanna rangeland', 'Feature_Type_Anthropogenic Feature',\n",
    "    'Feature_Type_Microfeature',\n",
    "    'horcolorvflag_80', 'horcolorvflag_90', 'horcolorvflag_100', 'horcolorvflag_110', 'horcolorvflag_120',\n",
    "    'obsmethod_50_Large Pit or Quarry',\n",
    "    'hzname_60_2Bt2', 'hzname_40_E',\n",
    "    'obsmethod_10_Shovel Slice', 'desgnmaster_60_O', 'hzname_0_Oa', 'rupresblkmst_50_Extremely firm', \n",
    "    'obsmethod_60_Trench', 'hzname_120_Bk', 'obsmethod_0_Cut', 'desgnmaster_90_O', 'obsmethod_20_Cut',\n",
    "     'hzname_110_2C2', 'obsmethod_70_Cut', 'effclass_60_Very slightly effervescent', 'obsmethod_30_Shovel Slice', \n",
    "     'obsmethod_90_Screw Auger', 'obsmethod_10_Large Pit or Quarry', 'desgnmaster_30_AB', \n",
    "     'effclass_30_Very slightly effervescent', 'hzname_50_2Bt2', 'desgnmaster_60_E', \n",
    "     'effclass_50_Very slightly effervescent', 'obsmethod_50_Cut', 'obsmethod_120_Cut', 'obsmethod_110_Trench', \n",
    "     'obsmethod_20_Large Pit or Quarry', 'obsmethod_40_Shovel Slice', 'obsmethod_10_Trench', 'hzname_30_BC',\n",
    "     'obsmethod_120_Trench', 'obsmethod_0_Shovel Slice', 'effclass_110_Very slightly effervescent', \n",
    "     'obsmethod_30_Large Pit or Quarry', 'desgnmaster_80_O', 'obsmethod_40_Cut', 'obsmethod_120_Large Pit or Quarry',\n",
    "     'effclass_40_Very slightly effervescent', 'rupresblkmst_60_Extremely firm', 'desgnmaster_50_O', 'hzname_40_Bt3',\n",
    "     'hzname_10_R', 'hzname_30_BA', 'hzname_70_Btk', 'obsmethod_20_Shovel Slice', 'obsmethod_70_Trench',\n",
    "     'obsmethod_0_Trench', 'obsmethod_60_Cut', 'obsmethod_40_Large Pit or Quarry', 'hzname_40_Btk', \n",
    "     'desgnmaster_40_O', 'obsmethod_80_Trench', 'rupresblkmst_0_Very firm', 'desgnmaster_70_E', 'desgnmaster_20_BE', \n",
    "     'obsmethod_100_Trench', 'hzname_80_Bw', 'obsmethod_100_Cut', 'hzname_30_Cr', \n",
    "     'effclass_80_Very slightly effervescent', 'hzname_110_Cg', 'hzname_80_C3', 'hzname_30_AB', \n",
    "     'obsmethod_90_Trench', 'obsmethod_90_Cut', 'desgnmaster_10_R', 'obsmethod_30_Trench', 'hzname_50_A', \n",
    "     'boundtopo_0_Irregular', 'effclass_120_Very slightly effervescent', 'obsmethod_80_Large Pit or Quarry', \n",
    "     'effclass_90_Very slightly effervescent', 'obsmethod_40_Trench', 'hzname_0_C1',\n",
    "     'obsmethod_110_Large Pit or Quarry', 'hzname_70_C3', 'bounddistinct_40_Diffuse', \n",
    "     'obsmethod_70_Large Pit or Quarry', 'desgnmaster_120_A', 'obsmethod_10_Cut', 'obsmethod_80_Cut', \n",
    "     'obsmethod_60_Large Pit or Quarry', 'obsmethod_110_Cut', 'effclass_100_Very slightly effervescent', \n",
    "     'obsmethod_80_Screw Auger', 'obsmethod_20_Trench', 'obsmethod_100_Large Pit or Quarry',\n",
    "     'obsmethod_50_Shovel Slice', 'obsmethod_10_OTHER', 'desgnmaster_30_BA', 'obsmethod_60_OTHER', \n",
    "     'obsmethod_0_Large Pit or Quarry', 'hzname_50_Btk', 'bounddistinct_30_Diffuse', 'desgnmaster_110_A', \n",
    "     'hzname_120_Bt3', 'hzname_80_2Bt3', 'obsmethod_90_Large Pit or Quarry', 'obsmethod_50_Trench',\n",
    "     'rupresblkmst_70_Extremely firm', 'bounddistinct_20_Diffuse', 'desgnmaster_70_O', 'hzname_50_A2', \n",
    "     'hzname_100_Bt', 'hzname_60_Btk', 'obsmethod_100_Screw Auger', 'hzname_120_Bk2', \n",
    "     'effclass_70_Very slightly effervescent', 'obsmethod_30_Cut', 'pmkind_glaciolacustrine',\n",
    "     'pmorigin_cherty', 'pmorigin_dolomite', 'pmorigin_quartzite', 'pmorigin_schist', 'pmorigin_volcanic', \n",
    "     'phfield_0_strong_alkaline', 'PlantName_Acer saccharum', 'PlantName_Aristida', \n",
    "     'PlantName_Calamagrostis canadensis', 'PlantName_Calamagrostis rubescens', 'PlantName_Chrysothamnus', \n",
    "     'PlantName_Elymus elymoides', 'PlantName_Koeleria macrantha', 'PlantName_Lupinus', \n",
    "     'PlantName_Pascopyrum smithii', 'PlantName_Pinus contorta', 'PlantName_Pinus ponderosa', \n",
    "     'PlantName_Pinus strobus', 'PlantName_Poa', 'PlantName_Populus tremuloides', \n",
    "     'PlantName_Prosopis glandulosa var. torreyana', 'PlantName_Pseudoroegneria spicata', \n",
    "     'PlantName_Quercus alba', 'PlantName_Quercus rubra', 'PlantName_Sporobolus cryptandrus',\n",
    "     'PlantName_Symphoricarpos albus', \n",
    "     'texture_60_VFSL', 'texture_0_GR-SIL', 'texture_90_LFS', 'texture_100_VFSL', 'texture_90_VFSL', \n",
    "     'texture_0_GRV-L', 'texture_120_LS', 'texture_30_GRV-L', 'texture_120_VFSL', 'texture_110_LFS',\n",
    "     'texture_50_VFSL', 'texture_10_GRV-L', 'texture_40_VFSL', 'texture_100_LFS', 'texture_0_GR-SL',\n",
    "     'texture_120_LFS', 'texture_20_GRV-L', 'texture_60_LFS', 'texture_70_LFS', 'texture_10_GR-SIL',\n",
    "     'texture_80_LFS', 'texture_110_VFSL', 'texture_30_GR-L', 'texture_80_VFSL', 'texture_70_VFSL']\n",
    "\n",
    "modeling_data.drop(columns=less_freq,inplace=True)\n",
    "modeling_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35171f",
   "metadata": {},
   "source": [
    "### Drop Index variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8bf5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first we impute the NaN values with kmeans (based on location because climate is location and elev deopendent)\n",
    "vars_to_impute = ['ppt01', 'ppt02', 'ppt03', 'ppt04', 'ppt05', 'ppt06', 'ppt07', 'ppt08', 'ppt09', 'ppt10', 'ppt11', 'ppt12', 'pptannual', \n",
    "\n",
    "'tdmean01', 'tdmean02', 'tdmean03', 'tdmean04', 'tdmean05', 'tdmean06', 'tdmean07', 'tdmean08', 'tdmean09', 'tdmean10', 'tdmean11', 'tdmean12', 'tdmeanannual',\n",
    "\n",
    "'tmax01', 'tmax02', 'tmax03', 'tmax04', 'tmax05', 'tmax06', 'tmax07', 'tmax08', 'tmax09', 'tmax10', 'tmax11', 'tmax12', 'tmaxannual',\n",
    "\n",
    "'tmean01', 'tmean02', 'tmean03', 'tmean04', 'tmean05', 'tmean06', 'tmean07', 'tmean08', 'tmean09', 'tmean10', 'tmean11', 'tmean12', 'tmeanannual',\n",
    "\n",
    "'tmin01', 'tmin02', 'tmin03', 'tmin04', 'tmin05', 'tmin06', 'tmin07', 'tmin08', 'tmin09', 'tmin10', 'tmin11', 'tmin12', 'tminannual',\n",
    "                  'vpdmaxannual',\n",
    "\n",
    "'vpdmin01', 'vpdmin02', 'vpdmin03', 'vpdmin04', 'vpdmin05', 'vpdmin06', 'vpdmin07', 'vpdmin08', 'vpdmin09', 'vpdmin10', 'vpdmin11', 'vpdmin12', 'vpdminannual']\n",
    "\n",
    "knn_regress_model = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "\n",
    "features = ['latstddeci','longstddec','elev']\n",
    "for var in vars_to_impute:\n",
    "    print(var)\n",
    "    target = var\n",
    "    knn_regress_model.fit(X = modeling_data.loc[modeling_data[target].notna(),features], \n",
    "                          y = modeling_data.loc[modeling_data[target].notna(),target])\n",
    "    \n",
    "    \n",
    "    modeling_data.loc[modeling_data[target].isnull(), target] = knn_regress_model.predict(\n",
    "                                modeling_data[features])[modeling_data[target].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb53cc",
   "metadata": {},
   "source": [
    "### Here are new climate variables to replace monthly variables:\n",
    "    tdm_nov_to_apr = mean (tdmean11, tdmean12, tdmean01, tdmean02, tdmean03, tdmean04)\n",
    "    tdm_may_to_oct = mean(tdmean05 ... tdmean10)\n",
    "    ppt_may_to_sep = mean(ppt05 ... ppt09)\n",
    "    ppt_oct_to_apr = mean(ppt10,ppt11,ppt12,ppt01,ppt02,ppt03,ppt04)\n",
    "    tmax_apr_to_sep = mean(tmax04 ... tmax09)\n",
    "    tmax_oct_to_mar = mean(tmax10,tmax11,tmax12,tmax01,tmax02,tmax03)\n",
    "    tmin_apr_to_oct = mean(tmin04 ... tmin10)\n",
    "    tmin_nov_to_mar = mean(tmin11, tmin12, tmin01, tmin02, tmin03)\n",
    "    tmean_apr_to_oct = mean(tmean04 ... tmean10)\n",
    "    tmean_nov_to_mar = mean(tmean11,tmean12,tmean01,tmean02,tmean03)\n",
    "    vpdmin_jun_to_oct = mean(vpd06 ... vpd10)\n",
    "    just drop vpdmax01 ... vpdmax12 (index is basically the same as vpdmaxannual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a9e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data.drop(columns=['vpdmax01','vpdmax02','vpdmax03','vpdmax04',\n",
    "                             'vpdmax05','vpdmax06','vpdmax07','vpdmax08',\n",
    "                             'vpdmax09','vpdmax10','vpdmax11','vpdmax12'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6911a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_var_with_average(var_list,model_data):\n",
    "    avg = model_data[var_list].mean(axis=1)\n",
    "    model_data.drop(columns=var_list,inplace=True)\n",
    "    return(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data['tdm_nov_to_apr'] = replace_var_with_average(['tdmean11', \n",
    "                            'tdmean12', 'tdmean01', 'tdmean02', 'tdmean03', 'tdmean04'],modeling_data)\n",
    "modeling_data['tdm_may_to_oct'] = replace_var_with_average(['tdmean05', \n",
    "                            'tdmean06', 'tdmean07', 'tdmean08', 'tdmean09', 'tdmean10'],modeling_data)\n",
    "\n",
    "modeling_data['ppt_may_to_sep'] = replace_var_with_average(['ppt05','ppt06','ppt07','ppt08','ppt09'],modeling_data)\n",
    "modeling_data['ppt_oct_to_apr'] = replace_var_with_average(['ppt10','ppt11','ppt12',\n",
    "                                                             'ppt01','ppt02','ppt03','ppt04'],modeling_data)\n",
    "\n",
    "modeling_data['tmax_apr_to_sep'] = replace_var_with_average(['tmax04','tmax05','tmax06','tmax07',\n",
    "                                                              'tmax08','tmax09'],modeling_data)\n",
    "modeling_data['tmax_oct_to_mar'] = replace_var_with_average(['tmax10','tmax11','tmax12','tmax01',\n",
    "                                                              'tmax02','tmax03'], modeling_data)\n",
    "\n",
    "modeling_data['tmin_apr_to_oct'] = replace_var_with_average(['tmin04','tmin05','tmin06','tmin07',\n",
    "                                                              'tmin08','tmin09','tmin10'], modeling_data)\n",
    "modeling_data['tmin_nov_to_mar'] = replace_var_with_average(['tmin11', 'tmin12', 'tmin01', 'tmin02',\n",
    "                                                              'tmin03'], modeling_data)\n",
    "\n",
    "modeling_data['tmean_apr_to_oct'] = replace_var_with_average(['tmean04','tmean05','tmean06','tmean07', 'tmean08','tmean09','tmean10'], modeling_data)\n",
    "modeling_data['tmean_nov_to_mar'] = replace_var_with_average(['tmean11','tmean12','tmean01','tmean02',\n",
    "                                                               'tmean03'], modeling_data)\n",
    "\n",
    "modeling_data['vpdmin_jun_to_oct'] = replace_var_with_average(['vpdmin06','vpdmin07', 'vpdmin08',\n",
    "                                                               'vpdmin09','vpdmin10'],modeling_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a576363",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data.shape, modeling_data.siteiid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e4f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterGroups = pd.read_csv('Input Files/ClusterGroups.csv')\n",
    "ClusterGroups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390fbff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ClusterGroups.drop_duplicates(inplace=True)\n",
    "ClusterGroups.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data_with_groups = pd.merge(modeling_data,ClusterGroups,on='siteiid',how='left')\n",
    "modeling_data_with_groups.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0a15a",
   "metadata": {},
   "source": [
    "### Topographic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602f5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_df_rest = pd.read_csv('Input Files/siteiid_lat_long_topo_20210319.csv')\n",
    "\n",
    "VDPNED6_NEGNED6_df = topo_df_rest[['siteiid','VDPNED6','NEGNED6']].drop_duplicates()\n",
    "VDPNED6_NEGNED6_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da04a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_siteobs_df = site_map_ids[['siteobsiid','siteiid']].drop_duplicates()\n",
    "\n",
    "site_siteobs_df['siteobsiid'] = site_siteobs_df['siteobsiid'].astype(int)\n",
    "\n",
    "site_siteobs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbdb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedons_topo_df = pd.read_csv('Input Files/pedons_topo.csv')\n",
    "pedons_topo_df = pd.merge(pedons_topo_df[['siteobsiid',\n",
    "                        'CRVNED6','DEMNED6c','DVMNED6',\n",
    "                        'GESUSG6_NA','MRNNED6','POSNED6',\n",
    "                        'SLPNED6','TPINED6']],site_siteobs_df,on='siteobsiid',how='inner')\n",
    "pedons_topo_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d96995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pedons_topo_df = pedons_topo_df[['siteiid','CRVNED6','DEMNED6c','DVMNED6',\n",
    "                        'GESUSG6_NA','MRNNED6','POSNED6',\n",
    "                        'SLPNED6','TPINED6']].drop_duplicates()\n",
    "\n",
    "pedons_topo_df['siteiid'] = pedons_topo_df['siteiid'].astype(int)\n",
    "\n",
    "pedons_topo_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7456bf",
   "metadata": {},
   "source": [
    "#### Fix GESUS variable with its classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc77c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "surfacegeo = pd.read_csv('Input Files/surfacegeology_legend_final.csv')\n",
    "surfacegeo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364007e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick out just the GESUS_variables and Value columns\n",
    "surfacegeo_var = surfacegeo[['GESUS_variables', 'Value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f313db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace original GESUS variables with its text version\n",
    "pedons_topo_df_V2 = pd.merge(pedons_topo_df,\n",
    "                      surfacegeo_var,\n",
    "                      how='left',\n",
    "                      left_on=['GESUSG6_NA'],\n",
    "                      right_on=['Value'])\n",
    "pedons_topo_df_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe33cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encode GESUS_variables\n",
    "GESUS_one_hot_encode = pedons_topo_df_V2[['siteiid']].join(pd.get_dummies(pedons_topo_df_V2['GESUS_variables'])).groupby('siteiid').max().reset_index()\n",
    "GESUS_one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de316fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join one hot encoded variables back to dataset\n",
    "pedons_topo_df_final = pd.merge(pedons_topo_df_V2,\n",
    "                      GESUS_one_hot_encode,\n",
    "                      how='left',\n",
    "                      left_on=['siteiid'],\n",
    "                      right_on=['siteiid'])\n",
    "pedons_topo_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e45f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Value, GESUSG6_NA, GESUS_variables\n",
    "pedons_topo_df_final = pedons_topo_df_final.drop(columns=['Value', 'GESUSG6_NA', 'GESUS_variables'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b0635",
   "metadata": {},
   "source": [
    "#### merge data with topo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e4e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data_with_topography = pd.merge(modeling_data_with_groups,pedons_topo_df_final,on='siteiid',how='left')\n",
    "modeling_data_with_topography.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81615853",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data_with_topography_cmp = pd.merge(modeling_data_with_topography,VDPNED6_NEGNED6_df,\n",
    "                                              on='siteiid',how='left')\n",
    "modeling_data_with_topography_cmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210577ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_data_with_topography_cmp.head()\n",
    "# modeling_data_with_topography_cmp[['VDPNED6','NEGNED6']].isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaa0f81",
   "metadata": {},
   "source": [
    "#### Fill in NaN for surface geo one hot encoded variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2893ca3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "surfacegeo_missing_var_list = [\n",
    "'GESUS_alluvial_thick_sediments',\n",
    "'GESUS_alluvial_thin_sediments',\n",
    "'GESUS_coastal_zone_sendiments',\n",
    "'GESUS_colluvial_alluvial_sediments',\n",
    "'GESUS_colluvial_sediments_discontinuous',\n",
    "'GESUS_colluvial_sediments_loess_residual_thin',\n",
    "'GESUS_eolian_sediments_dunesand',\n",
    "'GESUS_eolian_sediments_highplains',\n",
    "'GESUS_eolian_sediments_loess',\n",
    "'GESUS_glacial_till_sediments_clayey',\n",
    "'GESUS_glacial_till_sediments_sandy',\n",
    "'GESUS_glacial_till_sediments_silty',\n",
    "'GESUS_glaciofluvial_icecontact_sediments',\n",
    "'GESUS_organic_rich_muck',\n",
    "'GESUS_other',\n",
    "'GESUS_proglacial_sediments_coarse_grained',\n",
    "'GESUS_proglacial_sediments_fine_grained',\n",
    "'GESUS_residual_materials_alluvial_sediments',\n",
    "'GESUS_residual_materials_bedrock',\n",
    "'GESUS_residual_materials_carbonate_rocks',\n",
    "'GESUS_residual_materials_fine_grained_sedimentary_rocks',\n",
    "'GESUS_residual_materials_fine_igneous_metamorphic_rocks',\n",
    "'GESUS_residual_materials_sedimentary_rocks',\n",
    "'GESUS_water']\n",
    "modeling_data_with_topography_cmp[surfacegeo_missing_var_list] = modeling_data_with_topography_cmp[\n",
    "    surfacegeo_missing_var_list].apply(lambda x: x.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea1a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data = modeling_data_with_topography_cmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d7771",
   "metadata": {},
   "source": [
    "### Drop Index variable \n",
    "some index variables we drop after applying pca on model data and keep the pca value instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602c7c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_pre_PCA_index_var = pd.read_excel('Input Files/VariablestoDropWhenUsingIndices.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_pre_PCA_vars = drop_pre_PCA_index_var['ExcludeVariables'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f682ba7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_with_model = modeling_data.columns[modeling_data.columns.isin(drop_pre_PCA_vars)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac8ad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(drop_pre_PCA_vars) - set(shared_with_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b161822",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data.drop(columns= shared_with_model,inplace=True)\n",
    "modeling_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a272b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_indexed_vars = pd.read_csv('Input Files/ModelDataIndexData.csv')\n",
    "pca_indexed_vars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af26a86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_indexed_vars.drop_duplicates(inplace=True)\n",
    "pca_indexed_vars.shape, pca_indexed_vars.siteiid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43665d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_climate_vars = pca_indexed_vars.columns[pca_indexed_vars.columns.str.contains('|'.join(['Temp',\n",
    "                                                                           'VPD',\n",
    "                                                                           'Precipitation',\n",
    "                                                                           'Dewpoint']))].tolist()\n",
    "pca_indexed_vars.drop(columns= drop_climate_vars,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data.drop_duplicates(inplace=True)\n",
    "modeling_data.shape, modeling_data.siteiid.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96489258",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data_with_pca_indexed = pd.merge(modeling_data,pca_indexed_vars,on='siteiid',how='left' )\n",
    "modeling_data_with_pca_indexed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de5406",
   "metadata": {},
   "source": [
    "### KNN Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data = modeling_data_with_pca_indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816f8acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data.to_csv('Saved Datasets/modeling_data_with_pca_indexed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling_data = modeling_data_with_pca_indexed#.copy()\n",
    "\n",
    "\n",
    "### these are the variables we replace with knn \n",
    "vars_to_impute = ['NDVI_5Pct', 'NDVI_IQR90', 'NDVI_95Pct', 'SATVI_5Pct', 'SATVI_IQR90', 'SATVI_95Pct',\n",
    "                  'CRVNED6','DEMNED6c','DVMNED6', 'MRNNED6', 'POSNED6', 'SLPNED6',\n",
    "                  'TPINED6', 'VDPNED6', 'NEGNED6']\n",
    "\n",
    "\n",
    "knn_regress_model = KNeighborsRegressor(n_neighbors=3)\n",
    "\n",
    "\n",
    "features = ['latstddeci','longstddec','elev']\n",
    "for var in vars_to_impute:\n",
    "    print(var)\n",
    "    target = var\n",
    "    knn_regress_model.fit(X = modeling_data.loc[modeling_data[target].notna(),features], \n",
    "                          y = modeling_data.loc[modeling_data[target].notna(),target])\n",
    "    \n",
    "    \n",
    "    modeling_data.loc[modeling_data[target].isnull(), target] = knn_regress_model.predict(\n",
    "                                modeling_data[features])[modeling_data[target].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029150fa",
   "metadata": {},
   "source": [
    "### Last Cleaning/Variable Drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207435dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This is the modeling dataset that goes into the K-Means clustering algorithm\n",
    "modeling_data.to_csv('Saved Datasets/modeling_data_afterKNN_2_17.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222458a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeling_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b84509",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_type_var = [x for x in list(modeling_data) if 'other' in x.lower()] + ['HorizonMasterOth0to10Index',\n",
    "'HorizonMasterOth10to30Index',\n",
    "'HorizonMasterOth40to70Index',\n",
    "'HorizonMasterOth80to100Index',\n",
    "'HorizonNameOth_30to120Index',\n",
    "'HorizonTextureOTH_0to60Index',\n",
    "'HorizonTextureOTH_60to120Index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7664119",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_type_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d43fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'pmgroupnam_OTHER',\n",
    "'taxonname_OTHER',\n",
    "'taxclname_OTHER',\n",
    "'earthcov_1_Other grass/herbaceous cover',\n",
    "'pmkind_OTHER',\n",
    "'pmorigin_OTHER',\n",
    "'Feature_other',\n",
    "'PlantName_Other',\n",
    "'HorizonMasterOth0to10Index',\n",
    "'HorizonMasterOth10to30Index',\n",
    "'HorizonMasterOth40to70Index',\n",
    "'HorizonMasterOth80to100Index',\n",
    "'HorizonNameOth_30to120Index',\n",
    "'HorizonTextureOTH_0to60Index',\n",
    "'HorizonTextureOTH_60to120Index',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579deb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
